- speaker: ""
  title: "Few-Shot Learning of Force-Based Motions From Demonstration Through Pre-training of Haptic Representation"
  statement: "Impact on Future Robot Applications and Research: Our proposed semi-supervised Learning from Demonstration (LfD) approach addresses the challenge of learning contact-rich deformable manipulation tasks from a limited number of demonstrations. We evaluated our approach on a wiping task, which serves as a force-based manipulation task where the motion depends on the physical properties of the wiping tool and the object being wiped. Unlike previous approaches that relied heavily on large amounts of demonstration data, our method decouples the learnt model into a haptic representation encoder and a motion generation decoder. This allows us to pre-train the encoder using unsupervised data and then use few-shot LfD to train the decoder, leveraging human expertise. Our work demonstrates that pre-training significantly improves the ability of the LfD model to recognise physical properties and generate desired wiping motions for unseen sponges, outperforming the LfD method without pre-training on the physical robot hardware using the KUKA iiwa robot arm. This work has significant implications for real-world applications such as cleaning and cooking tasks, as it offers a scalable solution for learning generalisable deformable manipulation skills with minimal human effort.

  Taskonomy Roadmap for Future Robot Manipulation Research: Looking ahead, our taskonomy roadmap for future robot manipulation research consists of three key directions. Firstly, we propose extending our current work to incorporate multi-modal sensing, including vision, tactile, force, and proximity sensing. This expansion will enable robots to perform a greater variety of tasks requiring adaptation of motions based on the varying properties of manipulated objects. Secondly, while our approach considers generalisation of manipulation tasks to various objects, we aim to explore generalisation in two axes: generalisation of a skill model to handle various objects and generalisation of an object model to be shared across different tasks. This broader perspective will contribute to the development of more generalisable manipulation systems. Lastly, we intend to evaluate the influence of data informativeness and quantity on learning generalisable manipulation skills. In our approach, data informativeness increases from unsupervised simulated data to unsupervised real data and demonstration data, with a corresponding increase in the cost of data collection. Unsupervised real data eliminates the issue of the sim2real gap; however, it is still more expensive to collect than unsupervised simulated data. It is interesting to evaluate how the informativeness and quantity of the data influence these three types of data in learning generalisable manipulation skills. We are interested in how the characteristics of each data impact the robot's performance to develop data collection strategies for more robust and adaptable manipulation systems."
- speaker: ""
  title: "Multimodal Diffusion Transformer: Learning Versatile Behavior from Multimodal Goals"
  statement: "The presented work contributes to the workshop's general goal of advancing sensorimotor skill learning for robot manipulation, particularly by addressing the critical challenge of scalability in imitation learning through the lens of partially labeled datasets. We believe, that the future of skill learning requires more, diverse training data. While language offers an intuitive interface for goal guidance, manually labeling all robot demonstrations is time-consuming and labor-intensive. Thus, we are interested in finding methods that improve learning from partially labeled datasets for better language-conditioned policy learning. Therefore, we collected uncurated robot play data, as its cheap and easy to collect. Next, we only need to label a few sequences and can learn a versatile language-conditioned policy in an effective manner. We belive that this approach is an effective way to scale our poliicies towards more versatile agents given enough data.

  By emphasizing the practical significance of learning from uncurated datasets with sparse language annotations, the paper proposes two self-supervised objectives to train policies, that can learn with few language labels effectively. Further, we introduce a novel imitation learning policy, called Multimodal Diffusion Transformer (MDT), that improves upon prior diffusion policy architectures and can deal with different goal modalities effectively. By demonstrating MDT's proficiency in learning versatile behavior from multimodal goals, including language and images, the work underscores the importance of efficient utilization of available data resources for building more adaptable and robust manipulation robots. Through its empirical validation and comprehensive evaluation across popular imitation learning benchmarks such as LIBERO and CALVIN, the paper presents new insights, that we believe are relevant for the workshop."
- speaker: ""
  title: "Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning"
  statement: "The presented work directly contributes to the central theme of the workshop by addressing key challenges in efficient policies in visuomotor skill learning for robot manipulation. Diffusion Policies have recently gained widespread adoption as a policy representation for imitation learning. In contrast to other representation methods, it has shown strong capabilities to generalize well given enough training data, as seen in Octo. The current trend of training large policies on diverse data such as OXE demands a significant increase in computation. Thus, we believe there is a need to explore efficient architectures. Related to this, one of the major drawbacks of diffusion policy is still the high computation cost of generation of new actions in several denoising steps. Further, current diffusion policies typically contain several hundred million parameters. Our work aims to explore novel architecture that makes diffusion policies more computationally efficient to improve downstream skill learning. Therefore we explore a Mixture-of-Experts architecture for Diffusion Policies, where only a subset of parameters are required during each forward pass. This allows us to train bigger architectures on large-scale datasets while having more efficient policies. We believe that efficiency is one important building block to scale current policies towards more diverse skill learning and our work contains insights that are interesting for the audience of this workshop."
- speaker: ""
  title: "Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models"
  statement: "Language is frequently used to instruct robots for manipulation tasks. A large amount of diverse language-annotated data is required for policies to exhit strong generalization capabilities. However, generating language annotations for demonstration datasets is expensive and not scalable. To address this, we present a framework that can label long-horizon demonstrations with language in a zero-shot manner without human intervention by leveraging current state-of-the-art foundation models."
- speaker: ""
  title: "Symmetry-aware Learning for Contact-rich Maniuplaion under Partial Observability"
  statement: "This study addresses learning peg-insertion tasks using a soft robot that can operate more safely and tolerate lower-frequency control signals than a rigid one. We consider a partially observable formulation and deep reinforcement learning in which the peg-to-hole pose can not be measured directly.

  Soft robots and partial observation learning will impact future robot research in which robots can perform contact-rich manipulation tasks under unstructured environments. Our roadmap for future robot manipulation research is to gradually make robots in various contact-rich tasks more robust against more severe uncertainty possible in daily life."
- speaker: ""
  title: "Learning Visuotactile Skills with Two Multifingered Hands"
  statement: "The bimanual dexterous tasks we study, such as steak serving and wine pouring, represent common yet intricate challenges that existing robotic systems still struggle to accomplish effectively. By developing a novel system capable of handling these complex manipulation tasks through end-to-end learning approaches, our research aims to push the boundaries of current robot capabilities. Successful demonstrations in this domain would increase confidence in the feasibility of deploying advanced robots for similar real-world applications and inspire further exploration of learning techniques.

  One of the most significant roadblocks impeding rapid progress in robot manipulation research is the lack of robust data collection infrastructure and large-scale datasets. Our work directly confronts this obstacle by proposing an integrated hardware and software solution that enables efficient large-scale data collection for bimanual dexterous robotic hands. This data pipeline also enables us to build multimodal datasets that cover rich sensory inputs and diverse action spaces specific to bimanual dexterous manipulation tasks.

  Looking ahead, our roadmap for future robot manipulation research prioritizes expanding the scope and complexity of tasks under investigation. For example, we aim to gradually venture into longer-horizon tasks that involving tool use, dynamic tasks with moving objects, and tasks requiring high-level reasoning or multi-modal inputs. In parallel, we will continue refining our end-to-end learning frameworks to enhance their ability to generalize across tasks, environments, and hardware platforms. This entails exploring novel neural network architectures, multi-task learning strategies, and techniques for transferring knowledge between distinct but related manipulation skills."

- speaker: ""
  title: "Learning Goal-Conditioned Diffusion Policy for Contact-Rich Bimanual Manipulation through Planning-Guided Data Synthesis"
  statement: "The success of behavior cloning (BC) in dexterous manipulation has largely limited to tasks that utilize only the robot end effectors rather than its full arm or full body. This is not surprising as it is challenging to demonstrate such skills through human teleoperation. To scale robot learning, we highlight the need for 1) leveraging simulation data 2) extending manipulation learning beyond (jaw- or claw-like) end-effector-based skills. In this work, we present a contact-rich bimanual manipulation task that is hard to teleoperate, and show we can learn a visuomotor policy through planning-guided data synthesis in simulation and zero-shot transfer the learned policy to hardware."
- speaker: ""
  title: "From Simple to Complex Skills: The Case of In-Hand Object Reorientation"
  statement: "This work shows the benefits of reusing pretrained motor skills for more complex tasks. It is motivated by the recent success in sim-to-real for dexterous manipulation, which shows promising results in various applications. However, tuning the reward and system identifications are notoriously hard and time consuming. We provide another way: reusing previous skills and compose them for future tasks.

  I believe in future robot learning, more and more mature motor skills will be avaialble, either made by the research community and industry. When we face a new task, we should build a hierarchy of existing controllers instead training another one from scratch."
- speaker: ""
  title: "A Robotic Skill Learning System Built Upon Diffusion Policies and Foundation Models"
  statement: >
    Our recent work "A Robotic Skill Learning System Built Upon Diffusion Policies and Foundation Models" (currently under review and available on arXiv) shows how to leverage Diffusion Policies to perform complex tasks such as contact-rich bottle opening and granular material scooping while using large pre-trained foundational models to select the right policy given the users request as well as checking if defined preconditions for the tasks are fulfilled before execution. With this, it shows a potential approach to the problem of the wide variety of tasks robots will encounter in unstructured environments and how giving the user the power to demonstrate specific tasks to build an ever-growing library of tasks that are specific to the user needs. This sidesteps the problem of having to develop universal skills that, while more robust than before, are still in high danger of breaking if they are deployed outside the learned distribution by having the user perform the demonstrations in their specific environment. We expect that these skill-based systems will be very relevant for the future of robotic manipulation as their capability will be expanded to more and more varieties and environments.
- speaker: ""
  title: "Online Estimation of Articulated Objects with Factor Graphs using Vision and Proprioceptive Sensing"
  statement: "The main impact we hope to have with this work is to bring attention to the community the benefits of incorporating proprioceptive sensing into manipulation tasks. In particular, we are interested in manipulating articulated objects. We show in our paper how we can achieve impressive estimation accuracy of unknown articulations from very small motions (after only 0.5 degrees of rotation opening a door our estimator has an average accuracy of 90%, after 1.0 degree of rotation this improves to 97%). This is possible because we fuse learned priors based on visual information with kinematic sensing from the robot joint encoders to estimate the articulation.

  In our work we tackle the problem of estimating an unknown articulated object which has no visual cues to indicate its articulation. Therefore, it is not possible to open using vision alone and any initial guess of the articulation parameters needs to be updated as new information is received. We believe there is currently is a gap in the literature where vision and proprioceptive sensing should be used together to estimate the properties of objects. This paper addresses the gap for manipulating articulated objects and presents experiments on objects that most current methods would struggle to open."
- speaker: ""
  title: "Generative Factor Chaining: Coordinated Manipulation with Diffusion-based Factor Graph"
  statement: >
    My study revolves around solving complicated long-horizon planning tasks using short-horizon knowledge. Such tasks necessitate understanding the long-horizon action dependencies required to achieve the objectives. My research simplifies this problem using the options framework, defining tasks with skills or parameterized high-level actions (such as "pick," "place," and "move"). Given these skills (i.e., proficiency in solving short-horizon tasks), can we devise an intuitive planning strategy to directly chain them during inference for any arbitrary task? This line of study is crucial for future applications, as real-world tasks typically extend beyond short horizons, and collecting data for all possible scenarios is impractical. For instance, if we gather sufficient data on tasks like "picking up a hammer" and "picking up a nail" (individually from various tasks) and "moving a grasped object," can we infer solutions for unseen problems like hammering a nail directly at inference? Another example involves determining how to grasp a cup when the ultimate goal (after several steps of long-horizon reasoning) is either (a) placing it in a microwave or (b) placing it in a box, with the former requiring grasping by the handle and the latter by the rim.
- speaker: ""
  title: "Avoid Everything: Model-Free Collision Avoidance with Expert-Guided Fine-Tuning"
  statement: >
    Real-world robotic systems today largely exist in industrial spaces. While these spaces may change, the roboticists and operators can often work in tandem to design the spaces, tasks, and algorithms to be successful. These settings can be considered "technocratic" because they can be designed, dictated, and run by a highly trained technical team. I believe that robotic systems will have the greatest societal value by moving into "democratic" spaces, i.e. those that are defined by general population. However, these environments can be messy or chaotic. For example, typical homes have a long-tail of feasible environment configurations, and robots designed to interact with these settings will require careful consideration of safety. Traditional techniques in robotics make strong assumptions on scene observability to guarantee safety, yet many home environments cannot be instrumented in such a way as to ensure accurate scene models.

    In the last few years, our community has produced a lot of stellar research focused on learning skills for manipulation. In particular, much of this work has focused on learning an end-to-end policy that can mimic an expert performing complex manipulation tasks. However, these works have focused primarily on performing tasks in uncluttered settings where environmental collisions are not a major concern. In these settings, methods such as inverse kinematic following or motion planning with low-fidelity collision model are appropriate. In this research, we have focused on building tools from another perspective--choosing to think explicitly about end-to-end safety as a building block of a robust end-to-end system.

    Moving forward, we hope to see systems build off of our safe techniques so that they can perform dexterous manipulation tasks in complex or partially observed environments. While this may mean directly stitching our method into a pipeline that reasons at a task level about end effector motion, it may also lead to fully end-to-end systems that reason about the desired task in tandem with the relative safety of the actions. In "democratic" spaces, it is often impossible to adequately perform a task while satisfying hard collision-avoidance constraints. We believe that future end-to-end solutions will prove to be the fruitful when there is a trade-off between physical safety and task completion. Our submission to this workshop demonstrates a method for trading off between task completion and safety, and we hope this idea will help our community explore how we create generalized manipulation behaviors in democratic spaces."