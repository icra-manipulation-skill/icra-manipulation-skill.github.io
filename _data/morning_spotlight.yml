- speaker: ""
  title: "THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation"
  statement: "Drawing on insights fromthe paper the task of evaluating generalization in robotic manipulation directly impacts future robot applications and research by emphasizing the critical need for robots to adapt to dynamic, real-world environments. The findings from this paper underscore the importance of developing robotic systems that can perform reliably across a wide range of environmental conditions and tasks, which is essential for their practical deployment in many areas before attempts to scale up robot learning. Therefore, the future direction for skill development in robotics should extend beyond merely accumulating or developing new skills to critically assess the generalization and robustness of these acquired skills."
- speaker: ""
  title: "KinScene: Model-Based Mobile Manipulation of Articulated Scenes"
  statement: "In long-term manipulation tasks involving articulated objects, the robot must reason about the resultant motion of each part and anticipate its impact on future actions. We highlight three key challenges:

  1. Exploration: To understand the objects' articulation properties, the robot must be able to self-explore and collect necessary clues for inference.
  2. Scene-level Articulation Mapping: The map should encompass essential information, including 3D models and objects' articulation properties, to facilitate long-horizon planning for sequential tasks.
  3. Manipulation and Planning: To accommodate various objects of different sizes and positions, the robot must strategically plan the actions of both its mobile base and arm to execute sequential tasks while considering the kinematic constraints.
  Each task represents a distinct research challenge, driving current robotics research to focus on individual components within the robotic pipeline. While progress has been made in specific aspects of articulation manipulation, there is a notable gap in research that integrates these capabilities for real-world evaluations. Our proposed framework fills this gap by providing a generalized approach for sequential articulation manipulation through autonomous exploration, scene mapping, object articulation detection, and sequential object manipulation planning.

"
- speaker: ""
  title: "RoboPack: Learning Tactile-Informed Dynamics Models for Dense Packing"
  statement: "In our research, we focus on utilizing non-prehensile box pushing and dense object packing tasks as exemplars to illustrate how robots can adeptly manipulate objects with uncertain physical properties, employing both visual and tactile sensing modalities. These tasks pose significant challenges as the robot must accurately estimate the physical attributes of objects, such as mass distribution and deformability, to accomplish the desired objectives. To address these challenges, we propose a novel approach for multimodal fusion aimed at enhancing world model learning. By pushing the boundaries of what tasks robots can accomplish, our work signifies a crucial advancement in the field of robotic manipulation.

  Looking ahead, we believe that the integration of structured world model learning with model-based planning, alongside the strategic utilization of multimodal sensing, holds immense promise for future robot manipulation research. The integration of structured world model learning and model-based planning provides a foundation for creating interpretable and generalizable learning systems, essential for navigating the complexities of real-world manipulation tasks. Similarly, leveraging multimodal sensing enables robots to tackle challenging manipulation tasks that are beyond the scope of single-modal sensing, particularly when visual observations are impeded by occlusions or environmental factors.

  Our proposed framework serves as a tangible demonstration of the synergistic benefits of integrating structured world model learning, model-based planning, and multimodal sensing. By showcasing its effectiveness in addressing demanding manipulation tasks, such as non-prehensile manipulation and manipulation in cluttered environments, we offer a roadmap for future research endeavors. This roadmap emphasizes the importance of expanding robots' capabilities through the harmonious integration of structured world model learning and multimodal sensing, thereby paving the way for the development of more versatile and adept manipulation systems."
- speaker: ""
  title: "Bilateral Control-Based Imitation Learning via Action Chunking with Transformer"
  statement: "In this project, Bilateral Control-Based Imitation Learning via Action Chunking with Transformer (Bi-ACT), we utilized a real-world 'Pick-and-Place' task on various objects to evaluate the robot's adaptability and precision. The robot was tasked with manipulating objects with diverse sizes, shapes, consistencies, weight distributions, and hardness levels. This approach allowed us to observe how the robot applied learning from a limited set of training data to manage untrained objects adaptively. Successful manipulation of both trained and untrained objects, as seen across interpolation and extrapolation datasets, demonstrates the robot's capability to generalize its learning. This indicates that the Bi-ACT model could be extended to a broader range of tasks beyond its initial training scope.

  Based on the insights gleaned from the Bi-ACT project, our roadmap for future robot manipulation research focuses on enhancing the robot's adaptability and precision. In terms of adaptability, similar to the current 'Pick-and-Place' task, we aim to enable robots to generalize simple tasks, extending their capabilities from varying the objects to modifying the placement locations. Since complex tasks are amalgamations of simpler ones, perfecting these fundamental tasks is crucial for the successful completion of more complex and intricate tasks. Selecting appropriate training data will allow the robot to tackle tasks in a generalized manner and adapt seamlessly to different environments. To enhance the robot's precision, we are committed to refining the model and its preprocessing mechanisms, with a specific emphasis on image data. Tackling challenges such as object recognition will enable the robot to accurately identify and concentrate on the most salient features within the expansive datasets available. Moreover, considering the dynamic nature of real-world environments and the inevitability of errors, it is essential for the robot to have the capability to assess the current state of a task and, if necessary, reinitiate the process to achieve the desired outcome. This strategic direction will guide our research towards robots that can understand the surrounding environments and act accordingly, leading to enhanced precision and contributing significantly to the advancement of robot manipulation research."
- speaker: ""
  title: "DITTO: Demonstration Imitation by Trajectory Transformation"
  statement: "One-shot learning from human demonstrations is an important task for robotic manipulation, as, compared to robotic demonstrations, it presents a pathway to utilizing large and diverse datasets of human videos.

  I would like to distinguish between algorithmic taskonomy, describing the various pre-training and auxiliary losses and, functional taskonomy (for lack of a better word) describing the variety of robotic tasks that are addressed by the algorithm and tested during the experimental evaluation.

  Our algorithmic roadmap is to learn object centric trajectories from more unstructured in-the-wild videos. In terms of functional tasks, the one shot imitation approach gives us lots of flexibility to address a wide range of tasks, however we would like to look at improving manipulation of articulated objects. For this, we are looking at improving the execution by making it more robust."
- speaker: ""
  title: "ScrewMimic: Bimanual Imitation from Human Videos with Screw Space Projection (Remote)"
  statement: "Manipulation in human environments often involves tasks that require coordinating the motion of two arms, e.g., opening a bottle, cutting a block in two pieces, or stirring a pot. Given that the general-purpose robots that we want are going to operate in an environment that has been designed for humans and with objects that are being used by humans, endowing robots with such bimanual manipulation capabilities is crucial. Our work is a promising step towards enabling robots to efficiently learn such complex bimanual manipulation tasks by watching human demonstrations and fine-tuning the actions through interaction.

  To come up with a taskonomy roadmap, future robot manipulation research should study what are the most important and frequent tasks performed by humans. One way of doing that would be to assess large-scale human video datasets in different environments to understand the distribution of tasks. Furthermore, currently, a lot of complex manipulation tasks take place in a relatively constrained environment. In my mind, we are slowly moving towards really in-the-wild pick-and-place tasks for robots. While we are much farther away from such in-the-wild manipulation capabilities for other complex tasks, future research should study how we can remove such constraints to enable truly in-the-wild manipulation."
- speaker: ""
  title: "GILD: Generalizable Imitation Learning with 3D Semantic Fields (Remote)"
  statement: "Imitation learning has shown remarkable capability in executing complex robotic manipulation tasks. However, existing frameworks often fall short in structured modeling of the environment, lacking explicit characterization of geometry and semantics, which limits their ability to generalize to unseen objects and layouts. To enhance the generalization capabilities of imitation learning agents, we introduce a novel framework in this work, incorporating explicit spatial and semantic information via 3D semantic fields. We begin by generating 3D descriptor fields from multi-view RGBD observations with the help of large foundational vision models. These high-dimensional descriptor fields are then converted into low-dimensional semantic fields, which aids in the efficient training of a diffusion-based imitation learning policy. The proposed method offers explicit consideration of geometry and semantics, enabling strong generalization capabilities in tasks that require category-level generalization, resolving geometric ambiguities, and attention to subtle geometric details. We evaluate our method across eight tasks involving articulated objects and instances with varying shapes and textures from multiple object categories. Our method proves its effectiveness by outperforming state-of-the-art imitation learning baselines on unseen testing instances by 57%. Additionally, we provide a detailed analysis and visualization to interpret the sources of performance gain and explain how our method can generalize to novel instances."
- speaker: ""
  title: "D3Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation (Remote)"
  statement: "We propose a representation using foundation vision models to facilitate zero-shot manipulation tasks."

- speaker: ""
  title: "Robot Air Hockey: A Manipulation Testbed for Robot Learning with Reinforcement Learning"
  statement: "Robot manipulation has made tremendous progress in recent years. By creating Robot Air Hockey, we aim to push the robot manipulation research frontier in two major ways.

  First, we explore the setting of dynamic object manipulation, which is understudied in the real world relative to quasistatic object manipulation because of many challenges. Compared to other dynamic robotics tasks (e.g. learning quadrupedal policies), it can be difficult to collect data for dynamic manipulation. Mainly, the low latency requirements and frequent (often human) resets required in these tasks bottleneck data’s quality and quantity. We hope that our system serves as a potential testbed and provides insights for future works where we address some of these challenges. Some of the insights we observed from creating a dynamic manipulation system are: 1) maintaining low latency is challenging, and 2) playing at a relatively high frame rate---20Hz or 50ms, even humans can struggle. We hope to follow up with experiments investigating human capabilities across different modalities of teleoperation. Furthermore, one of the most significant hurdles is the robot emergency-stopping, either because a learned policy takes cyclic actions that result in damaging resonant behavior, or because the robot jerks too quickly when changing direction, making the robot exceed its acceleration limits. Further investigation into action smoothing could provide higher-quality human demonstration data and offline policies.

  Furthermore, recent work in robot learning has mostly focused on learning from demonstrations, where the data provided are usually optimal trajectories from humans or internet-scale data used to train representations. This optimality assumption is safe in quasistatic or low interaction environments, but in dynamic object manipulation, human teleoperators can lack the skill necessary to provide high-quality demonstrations. Robot air hockey considers one type of data that is relatively understudied: in-domain low-reward interaction data. Currently, we have collected robot data via teleoperation, and we are also planning to collect robot interaction data autonomously soon. As a result, this will open up avenues for many algorithms that are not limited to learning from demonstrations. For instance, our system allows us to assess RL algorithms and offers the opportunity to assess many forms of RL, such as goal-conditioned, offline or sim-to-real methods, to name a few. Because of the suboptimality of demonstration data, RL is an ideal tool for this setting, as suggested by the results in this work where offline RL outperforms other learning methods such as behavior cloning. Since related work assessing RL directly on a physical robot, especially in a dynamic manipulation setting, is limited, this work offers offline RL assessment in a dynamic, interactive real-world set of tasks.

  Our roadmap for future robot manipulation research is to learn from suboptimal interaction data and solve dynamic manipulation tasks in the real world. Current approaches have achieved incredible performance in quasistatic manipulation tasks, thanks to their abilities to harness optimal demonstration data and understand the task dynamics. However, doing so for dynamic manipulation is much more difficult. Although the scale of large models and datasets increases, in-domain data for dynamic tasks is still hard to collect. Even if one can collect in-domain data for these tasks, collecting optimal data is difficult, let alone deriving an accurate dynamics model for the task. Therefore, we believe this system is a powerful potential testbed for applying RL in the real world where we envision achieving super-human level performance on challenging, dynamic manipulation tasks."

- speaker: ""
  title: "LocoMan: Advancing Versatile Quadrupedal Dexterity with Lightweight Loco-Manipulators"
  statement: "In this work, we introduce LocoMan, an innovative quadrupedal robot equipped with a dexterous manipulator, designed for performing versatile tasks in a variety of constrained environments. Our design includes a lightweight 3-DoF (Degrees of Freedom) manipulator that is mounted on the robot's leg, coupled with a comprehensive whole-body controller that accommodates all five operating modes. Through experiments, we demonstrate the potential of LocoMan in performing real-life tasks. Specifically, LocoMan achieves a diverse set of challenging dexterous loco-manipulation tasks in confined spaces, such as opening doors, plugging into sockets, picking objects in narrow and low-lying spaces, and bimanual manipulation. Looking ahead, we aim to enhance LocoMan's autonomy by incorporating learning-based approaches."

- speaker: ""
  title: "Physics-informed Neural Motion Planning on Constraint Manifolds"
  statement: "Constrained motion planning plays an important role in robot manipulation tasks. For example, opening doors and carrying a glass filled with water both require robots moving on a zero-volume constraint manifold in the configuration space. We propose to use neural networks to solve a partial differential equation to find the geodesic path on the constraint manifold. This approach provides an efficient motion planning strategy and it does not require demonstration paths for learning, thereby enhancing the robot's capacity for manipulation."