- speaker: ""
  title: "THE COLOSSEUM: A Benchmark for Evaluating Generalization for Robotic Manipulation"
  statement: "Drawing on insights fromthe paper the task of evaluating generalization in robotic manipulation directly impacts future robot applications and research by emphasizing the critical need for robots to adapt to dynamic, real-world environments. The findings from this paper underscore the importance of developing robotic systems that can perform reliably across a wide range of environmental conditions and tasks, which is essential for their practical deployment in many areas before attempts to scale up robot learning. Therefore, the future direction for skill development in robotics should extend beyond merely accumulating or developing new skills to critically assess the generalization and robustness of these acquired skills."
- speaker: ""
  title: "KinScene: Model-Based Mobile Manipulation of Articulated Scenes"
  statement: "In long-term manipulation tasks involving articulated objects, the robot must reason about the resultant motion of each part and anticipate its impact on future actions. We highlight three key challenges:

  1. Exploration: To understand the objects' articulation properties, the robot must be able to self-explore and collect necessary clues for inference.
  2. Scene-level Articulation Mapping: The map should encompass essential information, including 3D models and objects' articulation properties, to facilitate long-horizon planning for sequential tasks.
  3. Manipulation and Planning: To accommodate various objects of different sizes and positions, the robot must strategically plan the actions of both its mobile base and arm to execute sequential tasks while considering the kinematic constraints.
  Each task represents a distinct research challenge, driving current robotics research to focus on individual components within the robotic pipeline. While progress has been made in specific aspects of articulation manipulation, there is a notable gap in research that integrates these capabilities for real-world evaluations. Our proposed framework fills this gap by providing a generalized approach for sequential articulation manipulation through autonomous exploration, scene mapping, object articulation detection, and sequential object manipulation planning.

"
- speaker: ""
  title: "RoboPack: Learning Tactile-Informed Dynamics Models for Dense Packing"
  statement: "In our research, we focus on utilizing non-prehensile box pushing and dense object packing tasks as exemplars to illustrate how robots can adeptly manipulate objects with uncertain physical properties, employing both visual and tactile sensing modalities. These tasks pose significant challenges as the robot must accurately estimate the physical attributes of objects, such as mass distribution and deformability, to accomplish the desired objectives. To address these challenges, we propose a novel approach for multimodal fusion aimed at enhancing world model learning. By pushing the boundaries of what tasks robots can accomplish, our work signifies a crucial advancement in the field of robotic manipulation.

  Looking ahead, we believe that the integration of structured world model learning with model-based planning, alongside the strategic utilization of multimodal sensing, holds immense promise for future robot manipulation research. The integration of structured world model learning and model-based planning provides a foundation for creating interpretable and generalizable learning systems, essential for navigating the complexities of real-world manipulation tasks. Similarly, leveraging multimodal sensing enables robots to tackle challenging manipulation tasks that are beyond the scope of single-modal sensing, particularly when visual observations are impeded by occlusions or environmental factors.

  Our proposed framework serves as a tangible demonstration of the synergistic benefits of integrating structured world model learning, model-based planning, and multimodal sensing. By showcasing its effectiveness in addressing demanding manipulation tasks, such as non-prehensile manipulation and manipulation in cluttered environments, we offer a roadmap for future research endeavors. This roadmap emphasizes the importance of expanding robots' capabilities through the harmonious integration of structured world model learning and multimodal sensing, thereby paving the way for the development of more versatile and adept manipulation systems."
- speaker: ""
  title: "Bilateral Control-Based Imitation Learning via Action Chunking with Transformer"
  statement: "In this project, Bilateral Control-Based Imitation Learning via Action Chunking with Transformer (Bi-ACT), we utilized a real-world 'Pick-and-Place' task on various objects to evaluate the robot's adaptability and precision. The robot was tasked with manipulating objects with diverse sizes, shapes, consistencies, weight distributions, and hardness levels. This approach allowed us to observe how the robot applied learning from a limited set of training data to manage untrained objects adaptively. Successful manipulation of both trained and untrained objects, as seen across interpolation and extrapolation datasets, demonstrates the robot's capability to generalize its learning. This indicates that the Bi-ACT model could be extended to a broader range of tasks beyond its initial training scope.

  Based on the insights gleaned from the Bi-ACT project, our roadmap for future robot manipulation research focuses on enhancing the robot's adaptability and precision. In terms of adaptability, similar to the current 'Pick-and-Place' task, we aim to enable robots to generalize simple tasks, extending their capabilities from varying the objects to modifying the placement locations. Since complex tasks are amalgamations of simpler ones, perfecting these fundamental tasks is crucial for the successful completion of more complex and intricate tasks. Selecting appropriate training data will allow the robot to tackle tasks in a generalized manner and adapt seamlessly to different environments. To enhance the robot's precision, we are committed to refining the model and its preprocessing mechanisms, with a specific emphasis on image data. Tackling challenges such as object recognition will enable the robot to accurately identify and concentrate on the most salient features within the expansive datasets available. Moreover, considering the dynamic nature of real-world environments and the inevitability of errors, it is essential for the robot to have the capability to assess the current state of a task and, if necessary, reinitiate the process to achieve the desired outcome. This strategic direction will guide our research towards robots that can understand the surrounding environments and act accordingly, leading to enhanced precision and contributing significantly to the advancement of robot manipulation research."
- speaker: ""
  title: "DITTO: Demonstration Imitation by Trajectory Transformation"
  statement: "One-shot learning from human demonstrations is an important task for robotic manipulation, as, compared to robotic demonstrations, it presents a pathway to utilizing large and diverse datasets of human videos.

  I would like to distinguish between algorithmic taskonomy, describing the various pre-training and auxiliary losses and, functional taskonomy (for lack of a better word) describing the variety of robotic tasks that are addressed by the algorithm and tested during the experimental evaluation.

  Our algorithmic roadmap is to learn object centric trajectories from more unstructured in-the-wild videos. In terms of functional tasks, the one shot imitation approach gives us lots of flexibility to address a wide range of tasks, however we would like to look at improving manipulation of articulated objects. For this, we are looking at improving the execution by making it more robust."
- speaker: ""
  title: "Multimodal Diffusion Transformer: Learning Versatile Behavior from Multimodal Goals"
  statement: "The presented work contributes to the workshop's general goal of advancing sensorimotor skill learning for robot manipulation, particularly by addressing the critical challenge of scalability in imitation learning through the lens of partially labeled datasets. We believe, that the future of skill learning requires more, diverse training data. While language offers an intuitive interface for goal guidance, manually labeling all robot demonstrations is time-consuming and labor-intensive. Thus, we are interested in finding methods that improve learning from partially labeled datasets for better language-conditioned policy learning. Therefore, we collected uncurated robot play data, as its cheap and easy to collect. Next, we only need to label a few sequences and can learn a versatile language-conditioned policy in an effective manner. We belive that this approach is an effective way to scale our poliicies towards more versatile agents given enough data.

  By emphasizing the practical significance of learning from uncurated datasets with sparse language annotations, the paper proposes two self-supervised objectives to train policies, that can learn with few language labels effectively. Further, we introduce a novel imitation learning policy, called Multimodal Diffusion Transformer (MDT), that improves upon prior diffusion policy architectures and can deal with different goal modalities effectively. By demonstrating MDT's proficiency in learning versatile behavior from multimodal goals, including language and images, the work underscores the importance of efficient utilization of available data resources for building more adaptable and robust manipulation robots. Through its empirical validation and comprehensive evaluation across popular imitation learning benchmarks such as LIBERO and CALVIN, the paper presents new insights, that we believe are relevant for the workshop."
- speaker: ""
  title: "ScrewMimic: Bimanual Imitation from Human Videos with Screw Space Projection (Remote)"
  statement: "Manipulation in human environments often involves tasks that require coordinating the motion of two arms, e.g., opening a bottle, cutting a block in two pieces, or stirring a pot. Given that the general-purpose robots that we want are going to operate in an environment that has been designed for humans and with objects that are being used by humans, endowing robots with such bimanual manipulation capabilities is crucial. Our work is a promising step towards enabling robots to efficiently learn such complex bimanual manipulation tasks by watching human demonstrations and fine-tuning the actions through interaction.

  To come up with a taskonomy roadmap, future robot manipulation research should study what are the most important and frequent tasks performed by humans. One way of doing that would be to assess large-scale human video datasets in different environments to understand the distribution of tasks. Furthermore, currently, a lot of complex manipulation tasks take place in a relatively constrained environment. In my mind, we are slowly moving towards really in-the-wild pick-and-place tasks for robots. While we are much farther away from such in-the-wild manipulation capabilities for other complex tasks, future research should study how we can remove such constraints to enable truly in-the-wild manipulation."
- speaker: ""
  title: "GILD: Generalizable Imitation Learning with 3D Semantic Fields (Remote)"
  statement: "Imitation learning has shown remarkable capability in executing complex robotic manipulation tasks. However, existing frameworks often fall short in structured modeling of the environment, lacking explicit characterization of geometry and semantics, which limits their ability to generalize to unseen objects and layouts. To enhance the generalization capabilities of imitation learning agents, we introduce a novel framework in this work, incorporating explicit spatial and semantic information via 3D semantic fields. We begin by generating 3D descriptor fields from multi-view RGBD observations with the help of large foundational vision models. These high-dimensional descriptor fields are then converted into low-dimensional semantic fields, which aids in the efficient training of a diffusion-based imitation learning policy. The proposed method offers explicit consideration of geometry and semantics, enabling strong generalization capabilities in tasks that require category-level generalization, resolving geometric ambiguities, and attention to subtle geometric details. We evaluate our method across eight tasks involving articulated objects and instances with varying shapes and textures from multiple object categories. Our method proves its effectiveness by outperforming state-of-the-art imitation learning baselines on unseen testing instances by 57%. Additionally, we provide a detailed analysis and visualization to interpret the sources of performance gain and explain how our method can generalize to novel instances."
- speaker: ""
  title: "D3Fields: Dynamic 3D Descriptor Fields for Zero-Shot Generalizable Robotic Manipulation (Remote)"
  statement: "We propose a representation using foundation vision models to facilitate zero-shot manipulation tasks."
- speaker: ""
  title: "Scaling Robot Policy Learning via Zero-Shot Labeling with Foundation Models"
  statement: "Language is frequently used to instruct robots for manipulation tasks. A large amount of diverse language-annotated data is required for policies to exhit strong generalization capabilities. However, generating language annotations for demonstration datasets is expensive and not scalable. To address this, we present a framework that can label long-horizon demonstrations with language in a zero-shot manner without human intervention by leveraging current state-of-the-art foundation models."
- speaker: ""
  title: "Efficient Diffusion Transformer Policies with Mixture of Expert Denoisers for Multitask Learning"
  statement: "The presented work directly contributes to the central theme of the workshop by addressing key challenges in efficient policies in visuomotor skill learning for robot manipulation. Diffusion Policies have recently gained widespread adoption as a policy representation for imitation learning. In contrast to other representation methods, it has shown strong capabilities to generalize well given enough training data, as seen in Octo. The current trend of training large policies on diverse data such as OXE demands a significant increase in computation. Thus, we believe there is a need to explore efficient architectures. Related to this, one of the major drawbacks of diffusion policy is still the high computation cost of generation of new actions in several denoising steps. Further, current diffusion policies typically contain several hundred million parameters. Our work aims to explore novel architecture that makes diffusion policies more computationally efficient to improve downstream skill learning. Therefore we explore a Mixture-of-Experts architecture for Diffusion Policies, where only a subset of parameters are required during each forward pass. This allows us to train bigger architectures on large-scale datasets while having more efficient policies. We believe that efficiency is one important building block to scale current policies towards more diverse skill learning and our work contains insights that are interesting for the audience of this workshop."